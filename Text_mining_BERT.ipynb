{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the data into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the data into pandas dataframes\n",
    "# ---------- 2013 ----------\n",
    "train2013 = pd.read_excel(r'./downloaded - xslx transformed/twitter-2013train-A.xlsx')\n",
    "test2013 = pd.read_excel(r'./downloaded - xslx transformed/twitter-2013test-A.xlsx')\n",
    "dev2013 = pd.read_excel(r'./\\downloaded - xslx transformed/twitter-2013dev-A.xlsx')\n",
    "\n",
    "# ---------- 2014 ----------\n",
    "sarcasm2014 = pd.read_excel(r'./downloaded - xslx transformed/twitter-2014sarcasm-A.xlsx')\n",
    "test2014 = pd.read_excel(r'./downloaded - xslx transformed/twitter-2014test-A.xlsx')\n",
    "\n",
    "# ---------- 2015 ----------\n",
    "train2015 = pd.read_excel(r'./downloaded - xslx transformed/twitter-2015train-A.xlsx')\n",
    "test2015 = pd.read_excel(r'./downloaded - xslx transformed/twitter-2015test-A.xlsx')\n",
    "\n",
    "# ---------- 2016 ----------\n",
    "train2016 = pd.read_excel(r'./downloaded - xslx transformed/twitter-2016train-A.xlsx')\n",
    "test2016 = pd.read_excel(r'./downloaded - xslx transformed/twitter-2016test-A.xlsx')\n",
    "dev2016 = pd.read_excel(r'./downloaded - xslx transformed/twitter-2016train-A.xlsx')\n",
    "dev_test2016 = pd.read_excel(r'./downloaded - xslx transformed/twitter-2016devtest-A.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all training dataframes into one\n",
    "train_dataframes = [train2013, sarcasm2014, train2015, train2016]\n",
    "train_data = pd.concat(train_dataframes)\n",
    "\n",
    "# merge all dev dataframes into one\n",
    "dev_dataframes = [dev2013, dev2016, dev_test2016]\n",
    "dev_data = pd.concat(dev_dataframes)\n",
    "\n",
    "# merge all test dataframes into one\n",
    "test2016.pop('Column4')\n",
    "test_dataframes = [test2013, test2014, test2015, test2016]\n",
    "test_data = pd.concat(test_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns\n",
    "train_data = train_data.rename(columns={'Column1': 'id', 'Column2': 'label', 'Column3':'tweet'})\n",
    "dev_data = dev_data.rename(columns={'Column1': 'id', 'Column2': 'label', 'Column3':'tweet'})\n",
    "test_data = test_data.rename(columns={'Column1': 'id', 'Column2': 'label', 'Column3':'tweet'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the labels from categorical ('positive', 'negative', 'neutral') to numerical (1, -1, 0)\n",
    "train_data['label_encoded'] = train_data['label'].map({'positive': 1, 'negative': -1, 'neutral': 0})\n",
    "dev_data['label_encoded'] = dev_data['label'].map({'positive': 1, 'negative': -1, 'neutral': 0})\n",
    "test_data['label_encoded'] = test_data['label'].map({'positive': 1, 'negative': -1, 'neutral': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_lengths = [len(train_data.tweet.to_list()[i]) for i in range(len(train_data.tweet))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length (character length, not number of words) of tweets:  22\n",
      "Max length (character length, not number of words) of tweets:  200\n"
     ]
    }
   ],
   "source": [
    "# get the min-max lengths of the tweets \n",
    "print('Min length (character length, not number of words) of tweets: ', min(tweet_lengths))\n",
    "print('Max length (character length, not number of words) of tweets: ', max(tweet_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the range of tweet lengths in the train dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_vals = range(len(train_data.tweet))\n",
    "y_vals = tweet_lengths\n",
    "\n",
    "plt.scatter(x_vals,y_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visibly, the tweets have pretty varied lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into X and y\n",
    "X_train = train_data['tweet'].to_list()\n",
    "y_train = train_data['label_encoded'].to_list()\n",
    "\n",
    "X_dev = dev_data['tweet'].to_list()\n",
    "y_dev = dev_data['label_encoded'].to_list()\n",
    "\n",
    "X_test = test_data['tweet'].to_list()\n",
    "y_test = test_data['label_encoded'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a baseline model\n",
    "\n",
    "- As a pre-processing step for this model, use TF-IDF to vectorize our data\n",
    "- The classifier chosen for the baseline model is: Naive-Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bogda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Uncomment to download \"stopwords\"\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def text_preprocessing(tweet):\n",
    "    \"\"\"\n",
    "    - Lowercase the sentence\n",
    "    - Change \"'t\" to \"not\"\n",
    "    - Remove \"@name\"\n",
    "    - Isolate and remove punctuations except \"?\"\n",
    "    - Remove other special characters\n",
    "    - Remove stop words except \"not\" and \"can\"\n",
    "    - Remove trailing whitespace\n",
    "    \"\"\"\n",
    "    # lower-case the text of the tweet\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Change 't to 'not'\n",
    "    tweet = re.sub(r\"\\'t\", \" not\", tweet)\n",
    "    \n",
    "    # Remove @name\n",
    "    tweet = re.sub(r'(@.*?)[\\s]', ' ', tweet)\n",
    "    \n",
    "    # Isolate and remove punctuations except '?'\n",
    "    tweet = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', tweet)\n",
    "    tweet = re.sub(r'[^\\w\\s\\?]', ' ', tweet)\n",
    "    \n",
    "    # Remove some special characters\n",
    "    tweet = re.sub(r'([\\;\\:\\|•«\\n])', ' ', tweet)\n",
    "    \n",
    "    # Remove stopwords except 'not' and 'can'\n",
    "    tweet = \" \".join([word for word in tweet.split()\n",
    "                  if word not in stopwords.words('english')\n",
    "                  or word in ['not', 'can']])\n",
    "    \n",
    "    # Remove trailing whitespace\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Preprocess text\n",
    "X_train_preprocessed = np.array([text_preprocessing(text) for text in X_train])\n",
    "X_dev_preprocessed = np.array([text_preprocessing(text) for text in X_dev])\n",
    "X_test_preprocessed = np.array([text_preprocessing(text) for text in X_test])\n",
    "\n",
    "# TF-IDF\n",
    "tf_idf = TfidfVectorizer(# the number of words to capture (unigrams, bigrams, trigrams)\n",
    "                         ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         # avoid division by 0\n",
    "                         smooth_idf=False)\n",
    "\n",
    "X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\n",
    "X_dev_tfidf = tf_idf.transform(X_dev_preprocessed)\n",
    "X_test_tfidf = tf_idf.transform(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5503483217226093"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# create the model\n",
    "baseline_model = MultinomialNB()\n",
    "\n",
    "# fit the data to the model\n",
    "baseline_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# get the score\n",
    "baseline_model.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multinomial Naive-Bayes algorithm returns a mean accuracy of 55.03%. \n",
    "\n",
    "The above method is the same as doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5503483217226093\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "baseline_predictions = baseline_model.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_s = accuracy_score(baseline_predictions, y_test)\n",
    "\n",
    "print(accuracy_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 16259\n",
      "Dev dataset size: 9654\n",
      "Test dataset size: 28422\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset size:', X_train_preprocessed.shape[0])\n",
    "print('Dev dataset size:', X_dev_preprocessed.shape[0])\n",
    "print('Test dataset size:', X_test_preprocessed.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the tweets from Vocabulary to int (Tokenize the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the top 5000 most common words\n",
    "train_tokenizer = Tokenizer(num_words = 5000, split = ' ')\n",
    "train_tokenizer.fit_on_texts(X_train_preprocessed)\n",
    "\n",
    "X_train_seq = train_tokenizer.texts_to_sequences(X_train_preprocessed)\n",
    "X_train_seq = pad_sequences(X_train_seq, maxlen = 100)\n",
    "\n",
    "# repeat for dev and test sets\n",
    "dev_tokenizer = Tokenizer(num_words = 5000, split = ' ')\n",
    "dev_tokenizer.fit_on_texts(X_dev_preprocessed)\n",
    "\n",
    "X_dev_seq = dev_tokenizer.texts_to_sequences(X_dev_preprocessed)\n",
    "X_dev_seq = pad_sequences(X_dev_seq, maxlen = 100)\n",
    "\n",
    "test_tokenizer = Tokenizer(num_words = 5000, split = ' ')\n",
    "test_tokenizer.fit_on_texts(X_test_preprocessed)\n",
    "\n",
    "X_test_seq = test_tokenizer.texts_to_sequences(X_test_preprocessed)\n",
    "X_test_seq = pad_sequences(X_test_seq, maxlen = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the labels\n",
    "train_enc = LabelEncoder()\n",
    "train_data['label_encoded2'] = train_enc.fit_transform(train_data['label'])\n",
    "\n",
    "dev_enc = LabelEncoder()\n",
    "dev_data['label_encoded2'] = dev_enc.fit_transform(dev_data['label'])\n",
    "\n",
    "test_enc = LabelEncoder()\n",
    "test_data['label_encoded2'] = test_enc.fit_transform(test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lstm = np.array(train_data['label_encoded2'])\n",
    "y_dev_lstm = np.array(dev_data['label_encoded2'])\n",
    "y_test_lstm = np.array(test_data['label_encoded2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune a BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_bert(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bert = np.array([text_preprocessing_bert(text) for text in X_train])            \n",
    "X_dev_bert = np.array([text_preprocessing_bert(text) for text in X_dev])\n",
    "X_test_bert = np.array([text_preprocessing_bert(text) for text in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data, MAX_LEN):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing_bert(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the maximum number of words in the tweets\n",
    "n_train = 0\n",
    "n_dev = 0\n",
    "n_test = 0\n",
    "\n",
    "for tweet in X_train_bert:\n",
    "    if n_train <= len(tweet.split()):\n",
    "        n_train = len(tweet.split())\n",
    "        \n",
    "for tweet in X_dev_bert:\n",
    "    if n_dev <= len(tweet.split()):\n",
    "        n_dev = len(tweet.split())\n",
    "        \n",
    "for tweet in X_test_bert:\n",
    "    if n_test <= len(tweet.split()):\n",
    "        n_test = len(tweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in the training tweets:  35\n",
      "Max number of words in the dev tweets:  34\n",
      "Max number of words in the test tweets:  33\n"
     ]
    }
   ],
   "source": [
    "print('Max number of words in the training tweets: ', n_train)\n",
    "print('Max number of words in the dev tweets: ', n_dev)\n",
    "print('Max number of words in the test tweets: ', n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  112\n"
     ]
    }
   ],
   "source": [
    "# put all tweets together\n",
    "all_tweets = np.concatenate([X_train_bert, X_dev_bert, X_test_bert])\n",
    "\n",
    "# Encode the tweets\n",
    "encoded_tweets = [tokenizer.encode(text, add_special_tokens=True) for text in all_tweets]\n",
    "\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_tweets])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the inputs and turn them into tensors\n",
    "X_train_inputs, X_train_mask = preprocessing_for_bert(X_train_bert, 100)\n",
    "X_dev_inputs, X_dev_mask = preprocessing_for_bert(X_dev_bert, 100)\n",
    "X_test_inputs, X_test_mask = preprocessing_for_bert(X_test_bert, 100)\n",
    "\n",
    "# turn the labels into tensors\n",
    "y_train_tensor = tf.convert_to_tensor(y_train_lstm)\n",
    "y_dev_tensor = tf.convert_to_tensor(y_dev_lstm)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfb8ceb8ebc4c63a80e5fe48021e6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=536063208.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel, BertConfig, TFBertForSequenceClassification\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  2307      \n",
      "=================================================================\n",
      "Total params: 109,484,547\n",
      "Trainable params: 109,484,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "log_dir = r'D:\\Facultate\\Year 1\\Semester 1\\Text Mining\\A3\\bert_model'\n",
    "model_save_path = r'D:\\Facultate\\Year 1\\Semester 1\\Text Mining\\A3\\bert_model/bert_model_TM_A3.h5'\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath = model_save_path,\n",
    "                                       save_weights_only= True,\n",
    "                                       monitor = 'val_loss',\n",
    "                                       mode = 'min',\n",
    "                                       save_best_only = True),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir = log_dir)\n",
    "]\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon = 1e-08)\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer, metrics = [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_bert = model.fit([X_train_inputs, X_train_mask],\n",
    "                         y_train_tensor,\n",
    "                         batch_size = 32,\n",
    "                         epochs = 4,\n",
    "                         validation_data = ([X_dev_inputs, X_dev_mask], y_dev_tensor),\n",
    "                         callbacks = callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
